"""
Servicio de EvaluaciÃ³n de Escritura
====================================

Este mÃ³dulo evalÃºa documentos de escritura del estudiante y genera
reportes detallados sobre calidad, mejoras y comparaciones entre versiones.

Funcionalidades:
- Extrae texto de archivos TXT, PDF, DOCX
- Analiza gramÃ¡tica, ortografÃ­a, estructura, vocabulario
- Compara versiones anteriores para medir progreso
- Usa Gemini AI para anÃ¡lisis profundo
- Genera reportes con mÃ©tricas y recomendaciones
"""

import os
import json
import re
from datetime import datetime
from typing import Dict, Optional, Tuple
import google.generativeai as genai
from flask import current_app

# Importar extractores de texto existentes
try:
    from app.services.document_processing.pdf_extractor import PDFExtractor
    PDF_AVAILABLE = True
except ImportError:
    PDF_AVAILABLE = False
    print("âš ï¸  PDFExtractor no disponible")


class WritingEvaluator:
    """
    Evaluador de escritura con IA
    
    Extrae texto de documentos, analiza calidad de escritura,
    y genera reportes detallados con mÃ©tricas y recomendaciones.
    """
    
    @staticmethod
    def _configure_gemini():
        """Configura la API de Gemini"""
        api_key = os.environ.get('GEMINI_API_KEY')
        if not api_key:
            api_key = current_app.config.get('GEMINI_API_KEY')
        
        if not api_key:
            raise ValueError("GEMINI_API_KEY no configurada")
        
        genai.configure(api_key=api_key)
    
    @staticmethod
    def _get_model():
        """Obtiene el modelo de Gemini configurado"""
        preferred = os.environ.get('GEMINI_MODEL', 'gemini-2.5-flash')
        
        # Lista de modelos a intentar en orden (actualizados a versiones 2024-2025)
        models_to_try = [
            preferred,
            'gemini-2.5-flash',
            'gemini-2.5-pro',
            'gemini-2.0-flash',
            'gemini-flash-latest',
            'gemini-pro-latest',
            'gemini-exp-1206'
        ]
        
        # Eliminar duplicados manteniendo el orden
        models_to_try = list(dict.fromkeys(models_to_try))
        
        last_error = None
        for model_name in models_to_try:
            try:
                print(f"   Intentando modelo: {model_name}")
                model = genai.GenerativeModel(model_name)
                print(f"   âœ… Modelo seleccionado: {model_name}")
                return model
            except Exception as e:
                last_error = e
                print(f"   âš ï¸ Modelo {model_name} no disponible: {e}")
                continue
        
        # Si ninguno funciona, lanzar el Ãºltimo error
        raise Exception(f"No se pudo cargar ningÃºn modelo. Ãšltimo error: {last_error}")
    
    @staticmethod
    def extract_text(file_path: str) -> str:
        """
        Extrae texto de un archivo
        
        Soporta:
        - TXT: lectura directa
        - PDF: usa PDFExtractor si estÃ¡ disponible
        - DOCX: usa python-docx si estÃ¡ instalado
        
        Args:
            file_path: Ruta al archivo
            
        Returns:
            str: Texto extraÃ­do
        """
        ext = os.path.splitext(file_path)[1].lower()
        
        print(f"ðŸ“„ Extrayendo texto de {os.path.basename(file_path)} ({ext})")
        
        # Archivo de texto plano
        if ext in ['.txt', '.md']:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                text = f.read()
                print(f"  âœ… ExtraÃ­dos {len(text)} caracteres")
                return text
        
        # Archivo PDF
        elif ext == '.pdf':
            if not PDF_AVAILABLE:
                raise ValueError("PDFExtractor no disponible. Instala PyPDF2 o pdfplumber")
            
            extractor = PDFExtractor()
            text = extractor.extract_text(file_path)
            print(f"  âœ… ExtraÃ­dos {len(text)} caracteres del PDF")
            return text
        
        # Archivo DOCX
        elif ext == '.docx':
            try:
                from docx import Document
                doc = Document(file_path)
                text = '\n'.join([para.text for para in doc.paragraphs])
                print(f"  âœ… ExtraÃ­dos {len(text)} caracteres del DOCX")
                return text
            except ImportError:
                raise ValueError("python-docx no instalado. Usa: pip install python-docx")
        
        else:
            raise ValueError(f"Formato de archivo no soportado: {ext}")
    
    @staticmethod
    def calculate_basic_metrics(text: str) -> Dict:
        """
        Calcula mÃ©tricas bÃ¡sicas de texto
        
        MÃ©tricas:
        - Palabras totales
        - Oraciones
        - PÃ¡rrafos
        - Promedio de palabras por oraciÃ³n
        - Vocabulario Ãºnico
        - Palabras largas (>7 caracteres)
        
        Args:
            text: Texto a analizar
            
        Returns:
            dict: MÃ©tricas calculadas
        """
        # Contar palabras
        words = re.findall(r'\b\w+\b', text.lower())
        word_count = len(words)
        
        # Contar oraciones (aproximado)
        sentences = re.split(r'[.!?]+', text)
        sentences = [s.strip() for s in sentences if s.strip()]
        sentence_count = len(sentences)
        
        # Contar pÃ¡rrafos
        paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
        paragraph_count = len(paragraphs)
        
        # Vocabulario Ãºnico
        unique_words = set(words)
        vocabulary_size = len(unique_words)
        
        # Palabras largas (complejidad lÃ©xica)
        long_words = [w for w in words if len(w) > 7]
        long_word_count = len(long_words)
        
        # Promedio de palabras por oraciÃ³n
        avg_words_per_sentence = word_count / sentence_count if sentence_count > 0 else 0
        
        # Ãndice de legibilidad (Flesch simplificado)
        # Menor = mÃ¡s difÃ­cil, Mayor = mÃ¡s fÃ¡cil
        if sentence_count > 0 and word_count > 0:
            avg_syllables = sum(WritingEvaluator._count_syllables(w) for w in words) / word_count
            readability = 206.835 - 1.015 * avg_words_per_sentence - 84.6 * avg_syllables
            readability = max(0, min(100, readability))  # Clamp entre 0-100
        else:
            readability = 50
        
        return {
            'word_count': word_count,
            'sentence_count': sentence_count,
            'paragraph_count': paragraph_count,
            'vocabulary_size': vocabulary_size,
            'long_word_count': long_word_count,
            'avg_words_per_sentence': round(avg_words_per_sentence, 2),
            'vocabulary_richness': round(vocabulary_size / word_count * 100, 2) if word_count > 0 else 0,
            'readability_score': round(readability, 2)
        }
    
    @staticmethod
    def _count_syllables(word: str) -> int:
        """Cuenta sÃ­labas en una palabra (aproximado para espaÃ±ol)"""
        vowels = 'aeiouÃ¡Ã©Ã­Ã³ÃºÃ¼'
        word = word.lower()
        syllable_count = 0
        previous_was_vowel = False
        
        for char in word:
            is_vowel = char in vowels
            if is_vowel and not previous_was_vowel:
                syllable_count += 1
            previous_was_vowel = is_vowel
        
        return max(1, syllable_count)  # MÃ­nimo 1 sÃ­laba
    
    @staticmethod
    def evaluate_with_ai(text: str, previous_text: Optional[str] = None) -> Dict:
        """
        EvalÃºa el texto usando Gemini AI
        
        AnÃ¡lisis profundo:
        - GramÃ¡tica y ortografÃ­a con errores especÃ­ficos
        - Coherencia y cohesiÃ³n
        - Vocabulario y estilo
        - Estructura y organizaciÃ³n
        - AnÃ¡lisis de tono y formalidad
        - Nivel de complejidad
        - Sugerencias especÃ­ficas de correcciÃ³n
        - ComparaciÃ³n con versiÃ³n anterior (si existe)
        
        Args:
            text: Texto actual a evaluar
            previous_text: Texto de versiÃ³n anterior (opcional)
            
        Returns:
            dict: Reporte de evaluaciÃ³n con scores y recomendaciones detalladas
        """
        try:
            print("ðŸ¤– Evaluando con Gemini AI (AnÃ¡lisis Profundo)...")
            
            WritingEvaluator._configure_gemini()
            model = WritingEvaluator._get_model()
            
            # Construir prompt segÃºn si hay comparaciÃ³n o no
            if previous_text:
                prompt = f"""
Eres un profesor experto en redacciÃ³n y escritura acadÃ©mica con enfoque en correcciÃ³n detallada.

TAREA: EvalÃºa el progreso del estudiante comparando dos versiones de su escrito. Proporciona anÃ¡lisis EXHAUSTIVO.

VERSIÃ“N ANTERIOR:
{previous_text[:3000]}

VERSIÃ“N ACTUAL:
{text[:3000]}

FORMATO DE SALIDA (JSON):
{{
  "overall_score": 85,
  "grammar_score": 90,
  "coherence_score": 80,
  "vocabulary_score": 85,
  "structure_score": 88,
  "improvement_percentage": 15,
  
  "tone_analysis": "acadÃ©mico",
  "formality_score": 85,
  "complexity_level": "intermedio-avanzado",
  
  "specific_errors": [
    {{
      "type": "ortografÃ­a",
      "error": "havÃ­a",
      "correction": "habÃ­a",
      "location": "pÃ¡rrafo 2",
      "explanation": "La 'h' es necesaria en el verbo haber"
    }},
    {{
      "type": "concordancia",
      "error": "los datos muestra",
      "correction": "los datos muestran",
      "location": "pÃ¡rrafo 3",
      "explanation": "El verbo debe concordar en plural con 'datos'"
    }}
  ],
  
  "suggestions": [
    {{
      "category": "estructura",
      "suggestion": "Conecta el pÃ¡rrafo 2 y 3 con una transiciÃ³n",
      "example": "Por otro lado, ...",
      "priority": "alta"
    }},
    {{
      "category": "vocabulario",
      "suggestion": "Reemplaza 'cosa' por tÃ©rmino mÃ¡s especÃ­fico",
      "example": "elemento, aspecto, caracterÃ­stica",
      "priority": "media"
    }}
  ],
  
  "strengths": [
    "Mejor uso de conectores lÃ³gicos",
    "Vocabulario tÃ©cnico mÃ¡s preciso",
    "ArgumentaciÃ³n mÃ¡s sÃ³lida con ejemplos",
    "Estructura clara con introducciÃ³n-desarrollo-conclusiÃ³n"
  ],
  
  "weaknesses": [
    "Algunos errores de puntuaciÃ³n en oraciones largas",
    "PÃ¡rrafos desbalanceados (algunos muy largos)",
    "Uso repetitivo de 'sin embargo'"
  ],
  
  "improvements_made": [
    "CorrigiÃ³ 3 errores ortogrÃ¡ficos previos",
    "MejorÃ³ la introducciÃ³n con contexto mÃ¡s claro",
    "AÃ±adiÃ³ 2 ejemplos concretos en el desarrollo",
    "Redujo uso de muletillas ('entonces', 'pues')"
  ],
  
  "recommendations": [
    "Revisar uso de comas en oraciones compuestas",
    "Dividir pÃ¡rrafo 4 en dos partes temÃ¡ticas",
    "Variar conectores: 'no obstante', 'por el contrario'",
    "Agregar mÃ¡s datos o estadÃ­sticas para respaldar argumentos",
    "Revisar consistencia en tiempos verbales (presente vs pasado)"
  ],
  
  "summary": "El estudiante muestra una mejora significativa en su escritura. La estructura es mÃ¡s clara y los argumentos estÃ¡n mejor desarrollados. Los principales avances son en vocabulario y coherencia. Se recomienda enfocarse en la puntuaciÃ³n y en la variedad de conectores para alcanzar un nivel avanzado."
}}

REGLAS:
1. Responde ÃšNICAMENTE con el objeto JSON (sin ```json ni texto adicional)
2. Scores del 0-100 (100 = excelente)
3. tone_analysis: formal/informal/acadÃ©mico/tÃ©cnico/narrativo
4. formality_score: 0-100 (0=muy informal, 100=muy formal)
5. complexity_level: bÃ¡sico/intermedio/intermedio-avanzado/avanzado
6. specific_errors: mÃ­nimo 3, mÃ¡ximo 10 errores mÃ¡s relevantes
7. suggestions: mÃ­nimo 3 sugerencias prÃ¡cticas con ejemplos
8. improvement_percentage: % de mejora respecto a versiÃ³n anterior
9. SÃ© MUY ESPECÃFICO: localiza errores, da ejemplos concretos
10. Prioriza feedback ACCIONABLE que el estudiante pueda aplicar

GENERA LA EVALUACIÃ“N EXHAUSTIVA:
"""
            else:
                prompt = f"""
Eres un profesor experto en redacciÃ³n y escritura acadÃ©mica con enfoque en correcciÃ³n detallada.

TAREA: EvalÃºa la calidad del siguiente escrito del estudiante. Proporciona anÃ¡lisis EXHAUSTIVO con todos los detalles.

TEXTO A EVALUAR:
{text[:4000]}

FORMATO DE SALIDA (JSON):
{{
  "overall_score": 75,
  "grammar_score": 80,
  "coherence_score": 70,
  "vocabulary_score": 75,
  "structure_score": 78,
  
  "tone_analysis": "acadÃ©mico",
  "formality_score": 80,
  "complexity_level": "intermedio",
  
  "specific_errors": [
    {{
      "type": "ortografÃ­a",
      "error": "aver",
      "correction": "haber",
      "location": "pÃ¡rrafo 1, lÃ­nea 3",
      "explanation": "'Haber' como verbo auxiliar siempre lleva 'h'"
    }},
    {{
      "type": "puntuaciÃ³n",
      "error": "...idea sin embargo...",
      "correction": "...idea. Sin embargo, ...",
      "location": "pÃ¡rrafo 2",
      "explanation": "Los conectores entre oraciones requieren punto antes y coma despuÃ©s"
    }},
    {{
      "type": "concordancia",
      "error": "el grupo de estudiantes participaron",
      "correction": "el grupo de estudiantes participÃ³",
      "location": "pÃ¡rrafo 3",
      "explanation": "El nÃºcleo del sujeto es 'grupo' (singular)"
    }}
  ],
  
  "suggestions": [
    {{
      "category": "estructura",
      "suggestion": "Agregar pÃ¡rrafo de introducciÃ³n mÃ¡s desarrollado",
      "example": "Comenzar con contexto general antes de presentar la tesis",
      "priority": "alta"
    }},
    {{
      "category": "vocabulario",
      "suggestion": "Reemplazar palabras genÃ©ricas por tÃ©rminos especÃ­ficos",
      "example": "'aspecto' â†’ 'caracterÃ­stica', 'dimensiÃ³n', 'factor'",
      "priority": "alta"
    }},
    {{
      "category": "coherencia",
      "suggestion": "Usar mÃ¡s conectores entre pÃ¡rrafos",
      "example": "Agregar: 'Por consiguiente', 'En contraste', 'Adicionalmente'",
      "priority": "media"
    }},
    {{
      "category": "gramÃ¡tica",
      "suggestion": "Revisar uso de gerundios al inicio de oraciones",
      "example": "Evitar: 'Siendo importante...' â†’ Mejor: 'Como es importante...'",
      "priority": "media"
    }}
  ],
  
  "strengths": [
    "Ideas bien fundamentadas con ejemplos concretos",
    "Uso correcto de vocabulario tÃ©cnico en el tema",
    "Buena estructura de introducciÃ³n con tesis clara",
    "PÃ¡rrafos con longitud adecuada (5-7 oraciones)"
  ],
  
  "weaknesses": [
    "Faltan conectores entre pÃ¡rrafos 2 y 3",
    "3 errores de concordancia gÃ©nero-nÃºmero",
    "ConclusiÃ³n muy breve (solo 2 oraciones)",
    "Uso repetitivo de 'es importante' (aparece 5 veces)",
    "Falta de citas o referencias para respaldar afirmaciones"
  ],
  
  "recommendations": [
    "Usar mÃ¡s conectores lÃ³gicos: 'sin embargo', 'por lo tanto', 'ademÃ¡s'",
    "Revisar concordancia de gÃ©nero y nÃºmero en todos los pÃ¡rrafos",
    "Ampliar la conclusiÃ³n: incluir implicaciones y reflexiÃ³n final",
    "Agregar 2-3 ejemplos concretos en los argumentos principales",
    "Variar el vocabulario: evitar repeticiÃ³n de palabras clave",
    "Dividir oraciones muy largas (mÃ¡s de 25 palabras) en dos",
    "Incluir datos o estadÃ­sticas para fortalecer argumentos",
    "Revisar consistencia en tiempos verbales (presente/pasado)"
  ],
  
  "summary": "Un escrito sÃ³lido con ideas claras y bien fundamentadas. El autor demuestra conocimiento del tema y capacidad de argumentaciÃ³n. Los puntos fuertes son la estructura lÃ³gica y el uso de vocabulario tÃ©cnico. Sin embargo, necesita trabajo en: 1) coherencia entre pÃ¡rrafos (mÃ¡s conectores), 2) revisiÃ³n gramatical (concordancias), y 3) desarrollo de la conclusiÃ³n. Con estas mejoras, el texto alcanzarÃ­a un nivel avanzado."
}}

REGLAS:
1. Responde ÃšNICAMENTE con el objeto JSON (sin ```json ni texto adicional)
2. Scores del 0-100 (100 = excelente)
3. tone_analysis: formal/informal/acadÃ©mico/tÃ©cnico/narrativo/persuasivo
4. formality_score: 0-100 (0=muy informal, 100=muy formal)
5. complexity_level: bÃ¡sico/intermedio/intermedio-avanzado/avanzado
6. specific_errors: mÃ­nimo 3, mÃ¡ximo 12 errores concretos con ubicaciÃ³n
7. suggestions: mÃ­nimo 4 sugerencias con ejemplos especÃ­ficos
8. SÃ© MUY ESPECÃFICO: localiza errores, da ejemplos de correcciÃ³n
9. En recommendations: mÃ­nimo 6 recomendaciones prÃ¡cticas y accionables
10. En summary: anÃ¡lisis de 3-4 oraciones con fortalezas, debilidades y pasos a seguir

GENERA LA EVALUACIÃ“N EXHAUSTIVA:
"""
            
            print("  ðŸš€ Enviando a Gemini...")
            response = model.generate_content(prompt)
            print(f"  âœ… Respuesta recibida: {len(response.text)} caracteres")
            
            # Limpiar respuesta de marcadores de cÃ³digo
            clean_text = response.text.strip()
            
            # Remover bloques de cÃ³digo markdown si existen
            if clean_text.startswith("```"):
                # Buscar el primer { y el Ãºltimo }
                start = clean_text.find("{")
                end = clean_text.rfind("}") + 1
                if start != -1 and end > start:
                    clean_text = clean_text[start:end]
            
            # Intentar parsear JSON
            try:
                evaluation = json.loads(clean_text)
            except json.JSONDecodeError as e:
                # Si falla, intentar reparar JSON comÃºn
                print(f"âš ï¸  Error JSON, intentando reparar: {e}")
                
                # Buscar el JSON vÃ¡lido mÃ¡s largo
                import re
                json_match = re.search(r'\{[^}]*(?:\{[^}]*\}[^}]*)*\}', clean_text, re.DOTALL)
                if json_match:
                    clean_text = json_match.group(0)
                    evaluation = json.loads(clean_text)
                else:
                    raise
            
            print(f"  âœ… EvaluaciÃ³n completada - Score: {evaluation.get('overall_score', 'N/A')}/100")
            print(f"  ðŸ“Š Errores detectados: {len(evaluation.get('specific_errors', []))}")
            print(f"  ðŸ’¡ Sugerencias: {len(evaluation.get('suggestions', []))}")
            print(f"  ðŸŽ¯ Tono: {evaluation.get('tone_analysis', 'N/A')}")
            print(f"  ðŸ“ Formalidad: {evaluation.get('formality_score', 'N/A')}/100")
            
            return evaluation
        
        except json.JSONDecodeError as e:
            print(f"âŒ Error parseando JSON de Gemini: {e}")
            print(f"Respuesta raw: {response.text[:500]}")
            return WritingEvaluator._fallback_evaluation(text, previous_text)
        
        except Exception as e:
            print(f"âŒ Error en evaluaciÃ³n con IA: {e}")
            return WritingEvaluator._fallback_evaluation(text, previous_text)
    
    @staticmethod
    def _fallback_evaluation(text: str, previous_text: Optional[str] = None) -> Dict:
        """
        EvaluaciÃ³n bÃ¡sica de fallback si Gemini falla
        
        Usa mÃ©tricas heurÃ­sticas simples.
        """
        print("âš ï¸  Usando evaluaciÃ³n heurÃ­stica de fallback")
        
        metrics = WritingEvaluator.calculate_basic_metrics(text)
        
        # Calcular scores basados en mÃ©tricas
        grammar_score = min(100, metrics['readability_score'] + 20)
        vocabulary_score = min(100, metrics['vocabulary_richness'] * 2)
        structure_score = 70 if metrics['paragraph_count'] >= 3 else 50
        coherence_score = 65
        overall_score = (grammar_score + vocabulary_score + structure_score + coherence_score) / 4
        
        evaluation = {
            'overall_score': round(overall_score),
            'grammar_score': round(grammar_score),
            'coherence_score': round(coherence_score),
            'vocabulary_score': round(vocabulary_score),
            'structure_score': round(structure_score),
            'tone_analysis': 'neutro',
            'formality_score': 50,
            'complexity_level': 'intermedio',
            'specific_errors': [
                {
                    'type': 'sistema',
                    'error': 'AnÃ¡lisis de IA no disponible',
                    'correction': 'Revisar manualmente',
                    'location': 'N/A',
                    'explanation': 'Se requiere revisiÃ³n manual completa'
                }
            ],
            'suggestions': [
                {
                    'category': 'general',
                    'suggestion': 'Revisar gramÃ¡tica y ortografÃ­a con herramienta externa',
                    'example': 'Usar corrector ortogrÃ¡fico',
                    'priority': 'alta'
                },
                {
                    'category': 'coherencia',
                    'suggestion': 'Verificar coherencia entre pÃ¡rrafos',
                    'example': 'Agregar conectores lÃ³gicos',
                    'priority': 'alta'
                }
            ],
            'strengths': [
                f"Vocabulario rico con {metrics['vocabulary_size']} palabras Ãºnicas",
                f"Buena extensiÃ³n: {metrics['word_count']} palabras",
                f"Legibilidad adecuada: {metrics['readability_score']}/100"
            ],
            'weaknesses': [
                "EvaluaciÃ³n limitada (IA no disponible)",
                "Se recomienda revisiÃ³n manual completa",
                "No se pudieron detectar errores especÃ­ficos"
            ],
            'recommendations': [
                "Revisar gramÃ¡tica y ortografÃ­a manualmente",
                "Verificar coherencia entre pÃ¡rrafos",
                "Usar herramientas de correcciÃ³n adicionales",
                "Pedir retroalimentaciÃ³n a un profesor o tutor",
                "Leer en voz alta para detectar errores",
                "Revisar consistencia en tiempos verbales"
            ],
            'summary': f"EvaluaciÃ³n bÃ¡sica: {metrics['word_count']} palabras, {metrics['sentence_count']} oraciones, legibilidad {metrics['readability_score']}/100. Se requiere anÃ¡lisis mÃ¡s profundo con IA o revisiÃ³n manual."
        }
        
        # Si hay versiÃ³n previa, calcular mejora
        if previous_text:
            prev_metrics = WritingEvaluator.calculate_basic_metrics(previous_text)
            word_diff = metrics['word_count'] - prev_metrics['word_count']
            vocab_diff = metrics['vocabulary_size'] - prev_metrics['vocabulary_size']
            
            improvement = 0
            if word_diff > 0:
                improvement += 5
            if vocab_diff > 0:
                improvement += 10
            
            evaluation['improvement_percentage'] = improvement
            evaluation['improvements_made'] = [
                f"Palabras: {prev_metrics['word_count']} â†’ {metrics['word_count']} ({'+' if word_diff > 0 else ''}{word_diff})",
                f"Vocabulario: {prev_metrics['vocabulary_size']} â†’ {metrics['vocabulary_size']} ({'+' if vocab_diff > 0 else ''}{vocab_diff})"
            ]
        
        return evaluation
    
    @staticmethod
    def generate_report(
        current_file: str,
        previous_file: Optional[str] = None,
        metadata: Optional[Dict] = None
    ) -> Dict:
        """
        Genera reporte completo de evaluaciÃ³n
        
        Flujo:
        1. Extraer texto de archivo(s)
        2. Calcular mÃ©tricas bÃ¡sicas
        3. Evaluar con IA
        4. Combinar resultados
        5. Generar reporte final
        
        Args:
            current_file: Ruta al archivo actual
            previous_file: Ruta al archivo anterior (opcional)
            metadata: Datos adicionales (user_id, course_id, etc.)
            
        Returns:
            dict: Reporte completo con mÃ©tricas, evaluaciÃ³n y recomendaciones
        """
        print("=" * 80)
        print("ðŸ“Š GENERANDO REPORTE DE EVALUACIÃ“N DE ESCRITURA")
        print("=" * 80)
        
        # 1. Extraer texto
        current_text = WritingEvaluator.extract_text(current_file)
        previous_text = None
        
        if previous_file and os.path.exists(previous_file):
            print(f"\nðŸ“„ Comparando con versiÃ³n anterior...")
            previous_text = WritingEvaluator.extract_text(previous_file)
        
        # 2. Calcular mÃ©tricas bÃ¡sicas
        print(f"\nðŸ“ˆ Calculando mÃ©tricas bÃ¡sicas...")
        current_metrics = WritingEvaluator.calculate_basic_metrics(current_text)
        print(f"  âœ… Palabras: {current_metrics['word_count']}")
        print(f"  âœ… Oraciones: {current_metrics['sentence_count']}")
        print(f"  âœ… Vocabulario Ãºnico: {current_metrics['vocabulary_size']}")
        print(f"  âœ… Legibilidad: {current_metrics['readability_score']}/100")
        
        previous_metrics = None
        if previous_text:
            previous_metrics = WritingEvaluator.calculate_basic_metrics(previous_text)
            print(f"\nðŸ“Š ComparaciÃ³n con versiÃ³n anterior:")
            print(f"  Palabras: {previous_metrics['word_count']} â†’ {current_metrics['word_count']}")
            print(f"  Vocabulario: {previous_metrics['vocabulary_size']} â†’ {current_metrics['vocabulary_size']}")
        
        # 3. Evaluar con IA
        print(f"\nðŸ¤– Evaluando calidad con IA...")
        ai_evaluation = WritingEvaluator.evaluate_with_ai(current_text, previous_text)
        
        # 4. Generar reporte final
        report = {
            'evaluated_at': datetime.utcnow().isoformat(),
            'file_name': os.path.basename(current_file),
            'metrics': {
                'current': current_metrics,
                'previous': previous_metrics
            },
            'evaluation': ai_evaluation,
            'metadata': metadata or {}
        }
        
        print("\n" + "=" * 80)
        print(f"âœ… REPORTE GENERADO - Score General: {ai_evaluation.get('overall_score', 'N/A')}/100")
        print("=" * 80)
        
        return report
